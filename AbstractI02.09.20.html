<html><head><meta content="text/html; charset=UTF-8" http-equiv="content-type"><style type="text/css">ol{margin:0;padding:0}table td,table th{padding:0}.c1{background-color:#ffffff;color:#24292e;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:10pt;font-family:"Courier New";font-style:normal}.c21{margin-left:41pt;padding-top:13pt;text-indent:-18pt;padding-bottom:0pt;line-height:2.1818181818181817;orphans:2;widows:2;text-align:left}.c23{margin-left:41pt;padding-top:24pt;text-indent:-18pt;padding-bottom:0pt;line-height:2.1818181818181817;orphans:2;widows:2;text-align:left}.c7{background-color:#ffffff;color:#222635;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:11pt;font-family:"Arial";font-style:normal}.c13{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:11pt;font-family:"Arial";font-style:normal}.c0{color:#292929;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:11pt;font-family:"Arial";font-style:normal}.c16{color:#24292e;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:11pt;font-family:"Courier New";font-style:normal}.c2{color:#000000;font-weight:700;text-decoration:none;vertical-align:baseline;font-size:11pt;font-family:"Arial";font-style:normal}.c5{padding-top:0pt;padding-bottom:0pt;line-height:1.15;orphans:2;widows:2;text-align:center}.c9{padding-top:12pt;padding-bottom:12pt;line-height:1.0;orphans:2;widows:2;text-align:left}.c15{padding-top:0pt;padding-bottom:0pt;line-height:1.07;orphans:2;widows:2;text-align:left}.c18{padding-top:0pt;padding-bottom:0pt;line-height:1.15;orphans:2;widows:2;text-align:left}.c4{padding-top:12pt;padding-bottom:12pt;line-height:1.15;orphans:2;widows:2;text-align:left}.c14{font-weight:700;text-decoration:none;vertical-align:baseline;font-size:11pt;font-family:"Arial";font-style:normal}.c3{background-color:#ffffff;font-size:10pt;font-family:"Courier New";color:#24292e;font-weight:400}.c10{font-family:"Courier New";color:#222635;font-weight:400}.c19{font-family:"Courier New";color:#24292e;font-weight:400}.c24{max-width:468pt;padding:72pt 72pt 72pt 72pt}.c11{font-size:10pt;color:#24292e}.c17{color:#292929}.c20{font-style:italic}.c12{color:#222635}.c8{background-color:#ffffff}.c6{height:11pt}.c22{font-weight:700}.title{padding-top:0pt;color:#000000;font-size:26pt;padding-bottom:3pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}.subtitle{padding-top:0pt;color:#666666;font-size:15pt;padding-bottom:16pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}li{color:#000000;font-size:11pt;font-family:"Arial"}p{margin:0;color:#000000;font-size:11pt;font-family:"Arial"}h1{padding-top:20pt;color:#000000;font-size:20pt;padding-bottom:6pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h2{padding-top:18pt;color:#000000;font-size:16pt;padding-bottom:6pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h3{padding-top:16pt;color:#434343;font-size:14pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h4{padding-top:14pt;color:#666666;font-size:12pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h5{padding-top:12pt;color:#666666;font-size:11pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h6{padding-top:12pt;color:#666666;font-size:11pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;font-style:italic;orphans:2;widows:2;text-align:left}</style></head><body class="c8 c24"><p class="c18 c6"><span class="c2"></span></p><p class="c18 c6"><span class="c2"></span></p><p class="c5 c6"><span class="c2"></span></p><p class="c5"><span class="c2">Topic - </span></p><p class="c5 c6"><span class="c2"></span></p><p class="c5"><span class="c2">An Attempt to Deduce a Convolutional Neural Network Which Can Process</span></p><p class="c5"><span class="c2">Both Real Life Images and Hand Drawn Images.</span></p><p class="c5 c6"><span class="c2"></span></p><p class="c5"><span class="c2">Project By</span></p><p class="c5 c6"><span class="c2"></span></p><p class="c5"><span class="c2">Yudhajit Chowdhury</span></p><p class="c5"><span class="c2">Enrollment Number : 12017002002190</span></p><p class="c5"><span class="c2">Section - A</span></p><p class="c5"><span class="c2">Roll - 3</span></p><p class="c5 c6"><span class="c2"></span></p><p class="c5"><span class="c2">Saheb Ganguly</span></p><p class="c5"><span class="c2">Enrollment Number : 12017002002206</span></p><p class="c5"><span class="c2">Section - A</span></p><p class="c5"><span class="c2">Roll - 67</span></p><p class="c5 c6"><span class="c2"></span></p><p class="c5 c6"><span class="c2"></span></p><p class="c5 c6"><span class="c2"></span></p><p class="c5"><span class="c2">Under The Supervision Of :</span></p><p class="c5 c6"><span class="c2"></span></p><p class="c5"><span class="c2">Prof. Sumanta Chakraborty</span></p><p class="c5 c6"><span class="c2"></span></p><p class="c5 c6"><span class="c2"></span></p><p class="c5"><span class="c2">Department of Computer Science and Engineering </span></p><p class="c5"><span class="c2">Institute of Engineering and Management West Bengal, India</span></p><p class="c5 c6"><span class="c2"></span></p><p class="c5 c6"><span class="c2"></span></p><p class="c5"><span class="c2">02.09.20</span></p><p class="c5 c6"><span class="c2"></span></p><p class="c5 c6"><span class="c2"></span></p><p class="c5 c6"><span class="c2"></span></p><p class="c5 c6"><span class="c2"></span></p><p class="c6 c18"><span class="c2"></span></p><p class="c4 c6"><span class="c13"></span></p><p class="c4 c6"><span class="c2"></span></p><p class="c4 c6"><span class="c2"></span></p><p class="c4"><span class="c2">THIS IS THE FIRST ABSTRACT OF THE PROJECT.</span></p><p class="c4"><span class="c2">AS THIS PROJECT IS BASED ON DATA SCIENCE AND DUE TO LIMITED DATA, AND TIME FOR MINING AND CLEANING NEW DATA FROM DIFFERENT SOURCES ,SO THIS ABSTRACT IS ONLY A FIRST GLANCE OF FUTURE IDEAS AND IMPLEMENTATIONS. THIS ABSTRACT IS ALSO DESIGNED TO ADAPT THE INITIAL IDEA FOR THIS PROJECT.</span></p><p class="c4 c6"><span class="c2"></span></p><p class="c4"><span class="c22">INTRODUCTION : </span></p><p class="c4"><span class="c7">Humans have a lot of senses, and yet our sensory experiences are typically dominated by vision. With that in mind, perhaps it is unsurprising that the vanguard of modern machine learning has been led by computer vision tasks. Likewise, when humans want to communicate or receive information, the most ubiquitous and natural avenue they use is language. Language can be conveyed by spoken and written words, gestures, or some combination of modalities, &nbsp;but for the purposes of this project, we&rsquo;ll focus on the best combination of hyper parameter for a nicely accurate deep learning that can predict a tree by showing a picture of a real tree and implementing that same algorithm on a hand drawn model.</span></p><p class="c4"><span class="c17 c8">Deep learning has absolutely dominated computer vision over the last few years, achieving top scores on many tasks and their related competitions. The most popular and well known of these computer vision competitions is </span><span class="c8">ImageNet</span><span class="c0 c8">. The ImageNet competition tasks researchers with creating a model that most accurately classifies the given images in the dataset. Nearly every year since 2012 has given us big breakthroughs in developing deep learning models for the task of image classification. Due to its large scale and challenging data, the ImageNet challenge has been the main benchmark for measuring progress.</span></p><p class="c4 c6"><span class="c0 c8"></span></p><p class="c4 c6"><span class="c0 c8"></span></p><p class="c4"><span class="c14 c17 c8">IDEA (NOT YET IMPLEMENTED):</span></p><p class="c4"><span class="c0 c8">The problem of Image Classification goes like this: Given a set of images of trees that are all labeled with a category, we are asked to predict these categories for a novel set of test images and measure the accuracy of the predictions. There are a variety of challenges associated with this task, including viewpoint variation, scale variation, intra-class variation, image deformation, image occlusion, illumination conditions, background clutter etc.</span></p><p class="c4 c6"><span class="c0 c8"></span></p><p class="c4 c6"><span class="c0 c8"></span></p><p class="c4 c6"><span class="c0 c8"></span></p><p class="c4"><span class="c14 c17 c8">ALGORITHM(S) : </span></p><p class="c4"><span class="c0 c8">How might we go about writing an algorithm that can classify images into distinct categories?</span></p><p class="c4"><span class="c17 c8">Computer Vision researchers have come up with a data-driven approach to solve this. Instead of trying to specify what every one of the image categories of interest look like directly in code, &nbsp; provide the computer with many examples of each image class and then develop learning algorithms that look at these examples and learn about the visual appearance of each class. In other words, they first accumulate a training dataset of labeled images, then feed it to the computer in order for it to get familiar with the data</span><span class="c0 c8">.</span></p><p class="c4"><span class="c0 c8">With this project we can find the best combination of all the parameters and the hyperparameter to find the best model and accuracy.</span></p><p class="c15 c6"><span class="c0 c8"></span></p><p class="c15 c6"><span class="c0 c8"></span></p><p class="c15 c6"><span class="c0 c8"></span></p><p class="c15"><span class="c14 c17 c8">THEORETICAL IMPLEMENTATION : </span></p><p class="c4"><span class="c7">Training CNN Model:</span></p><p class="c4"><span class="c0">I will create a variety of different CNN-based classification models to evaluate performances on Fashion MNIST. I will be building our model using the Keras framework. For more information on the framework, you can refer to the documentation here. Here are the list of models I will try out and compare their results:</span></p><p class="c8 c23"><span class="c17">1.</span><span class="c17">&nbsp; &nbsp; &nbsp;</span><span class="c0">CNN with 1 Convolutional Layer</span></p><p class="c8 c21"><span class="c17">2.</span><span class="c17">&nbsp; &nbsp; </span><span class="c0">CNN with 3 Convolutional Layer</span></p><p class="c21 c8"><span class="c17">3.</span><span class="c17">&nbsp; &nbsp; </span><span class="c0">CNN with 4 Convolutional Layer</span></p><p class="c21 c8"><span class="c17">4.</span><span class="c17">&nbsp; &nbsp; </span><span class="c0">VGG-19 Pre-Trained Model</span></p><p class="c4"><span class="c7">&nbsp;</span></p><p class="c4 c6"><span class="c7"></span></p><p class="c4 c6"><span class="c7"></span></p><p class="c4"><span class="c7">And the steps that we are following are:</span></p><p class="c4"><span class="c0">&middot;Split the original training data (60,000 images) into 80% training (48,000 images) and 20% validation (12000 images) optimize the classifier, while keeping the test data (10,000 images) to finally evaluate the accuracy of the model on the data it has never seen. This helps to see whether I&rsquo;m over-fitting on the training data and whether I should lower the learning rate and train for more epochs if validation accuracy is higher than training accuracy or stop over-training if training accuracy shifts higher than the validation. &middot; &nbsp; &nbsp; &nbsp; &nbsp;</span></p><p class="c4"><span class="c0">Train the model for 10 epochs with batch size of 256, compiled with categorical_crossentropy loss function and Adam optimizer. &middot; &nbsp; &nbsp; &nbsp; &nbsp;</span></p><p class="c4"><span class="c17">Then, add data augmentation, which generates new training samples by rotating, shifting and zooming on the training samples, and train the model on updated data for another 50 epochs. After loading and splitting the data, I preprocess them by reshaping them into the shape the network expects and scaling them so that all values are in the [0, 1] interval. Previously, for instance, the training data were stored in an array of shape (60000, 28, 28) of type uint8 with values in the [0, 255] interval. I transform it into a float32 array of shape (60000, 28 * 28) with values between 0 and 1.</span><span class="c14 c12 c8">&nbsp;</span></p><p class="c4 c6"><span class="c14 c12 c8"></span></p><p class="c4 c6"><span class="c14 c12 c8"></span></p><p class="c4"><span class="c14 c12 c8">INITIAL REFERENCES : </span></p><p class="c4 c6"><span class="c12 c8 c14"></span></p><p class="c4"><span class="c10 c8">images.google.com </span><span class="c12 c8">- </span><span class="c7">Real Life Image Data Set Will Be Mined From Google Images.</span></p><p class="c4"><span class="c8 c10">Kaggle.com</span><span class="c12 c8 c20">&nbsp;- </span><span class="c7">HandDrawn Image Data Set Will Be Gathered. </span></p><p class="c4"><span class="c10 c8">github.com</span><span class="c12 c8">&nbsp;</span><span class="c20 c12 c8">- </span><span class="c7">Project Archive and Resource Gathering.</span></p><p class="c4 c6"><span class="c7"></span></p><p class="c4 c6"><span class="c7"></span></p><p class="c4 c6"><span class="c7"></span></p><p class="c4 c6"><span class="c7"></span></p><p class="c4 c6"><span class="c7"></span></p><p class="c4"><span class="c14 c12 c8">INITIAL METADATA : </span></p><p class="c4"><span class="c14 c12 c8">Real Life Image :</span></p><p class="c4"><span class="c12 c8">Provider : Google - &nbsp;</span><span class="c3">https://en.wikipedia.org/wiki/Google</span><span class="c14 c12 c8">&nbsp;</span></p><p class="c4"><span class="c7">License : NA</span></p><p class="c4"><span class="c14 c12 c8">Hand Drawn Image : </span></p><p class="c4"><span class="c8 c12">Provider : Google - &nbsp;</span><span class="c3">https://en.wikipedia.org/wiki/Google</span><span class="c14 c12 c8">&nbsp;</span></p><p class="c4"><span class="c12 c8">License : </span><span class="c8 c11">CC BY 4.0</span><span class="c3">&nbsp;</span><span class="c11 c8">-</span><span class="c3">&nbsp;</span><span class="c1">https://creativecommons.org/licenses/by/4.0/</span></p><p class="c4 c6"><span class="c1"></span></p><p class="c4 c6"><span class="c1"></span></p><p class="c4 c6"><span class="c1"></span></p><p class="c4 c6"><span class="c1"></span></p><p class="c4 c6"><span class="c1"></span></p><p class="c4 c6"><span class="c1"></span></p><p class="c4 c6"><span class="c1"></span></p><p class="c4 c6"><span class="c1"></span></p><p class="c4 c6"><span class="c1"></span></p><p class="c4 c6"><span class="c1"></span></p><p class="c4 c6"><span class="c1"></span></p><p class="c4 c6"><span class="c1"></span></p><p class="c4 c6"><span class="c1"></span></p><p class="c4 c6"><span class="c1"></span></p><p class="c4 c6"><span class="c1"></span></p><p class="c4 c6"><span class="c1"></span></p><p class="c4 c6"><span class="c1"></span></p><p class="c4 c6"><span class="c1"></span></p><p class="c9"><span class="c16 c8">SIGNATURE OF STUDENTS:</span></p><p class="c9"><span class="c16 c8">&nbsp;</span></p><p class="c9"><span class="c16 c8">1.</span></p><p class="c9"><span class="c16 c8">&nbsp;</span></p><p class="c9"><span class="c8 c16">2.</span></p><p class="c9"><span class="c16 c8">&nbsp;</span></p><p class="c9"><span class="c16 c8">SIGNATURE OF MENTORS: </span></p><p class="c9 c6"><span class="c16 c8"></span></p><p class="c9"><span class="c16 c8">1. </span></p><p class="c9 c6"><span class="c16 c8"></span></p><p class="c9"><span class="c8 c19">DATE : </span></p><p class="c4 c6"><span class="c1"></span></p><p class="c4 c6"><span class="c1"></span></p><p class="c5 c6"><span class="c2"></span></p><p class="c5"><span class="c2">&nbsp;</span></p><p class="c5 c6"><span class="c13"></span></p><p class="c5 c6"><span class="c13"></span></p></body></html>